{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48fe43d",
   "metadata": {},
   "source": [
    "<font size=3> This chapter will begin with an end-to-end example where we use a **model** and a **tokenizer** together to replicate the ```pipeline()``` function introduced in Chapter 1. \n",
    "    \n",
    "<font size=3> Next, we’ll discuss the model API: we’ll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions.\n",
    "\n",
    "<font size=3> Then we’ll look at the tokenizer API, which is the other main component of the pipeline() function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c07eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
